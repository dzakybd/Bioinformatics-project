\documentclass[a4paper,oneside]{article}
\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{newtxtext,newtxmath}
\usepackage[top=0.7in, bottom=0.7in, left = 0.7in, right = 0.7in]{geometry}
\usepackage{enumitem}
\setlist{nolistsep}
\usepackage{caption}
\setlength{\belowcaptionskip}{-10pt}
\let\OLDthebibliography\thebibliography
\renewcommand\thebibliography[1]{
  \OLDthebibliography{#1}
  \setlength{\parskip}{0pt}
  \setlength{\itemsep}{0pt plus 0.3ex}
}
\begin{document}
\title{\vspace{-0.7in}High-Grade Prostate Cancer Prediction using DNA Methylation Data}
\author{Fawwaz Dzaky Zakiyal (201899213)\\Tanjung Dion (201883621)\\}
\date{Bioinformatics (Fall 2018)}
\maketitle
 
\section{Introduction}
Prostate cancer affects a significant proportion of the male population, but in most cases, the disease is harmless. However, for the small portion of the population with high-grade prostate cancer, the disease can be extremely debilitating, causing painful symptoms and even death \cite{prostate}. Prior studies have been performed, showing how hereditary factors, alcohol consumption, sexual activity, family history, and race can take a role in developing high-grade prostate cancer. Over the past years, rapid advances in sequencing technology have led to The Cancer Genome Atlas (TCGA) project which provides the most comprehensive genomic data for various kinds of cancer. In the previous research, to indicates the classification of patient samples was done by using TCGA Exon expression or SNP datasets. However, the recent study shows that DNA methylation act as better bio-markers and help in improving the cancer prognosis \cite{dnameth}. Beside that, in the recent years, machine learning become useful tool for prediction and regression tasks. It could bring benefit for the cancer risk prediction. Thus, this project implement machine learning techniques to classify high-grade prostate cancer risk using DNA methylation data.


\section{Methodology}
\subsection{Concepts}
\begin{itemize}
\item \textbf{DNA methylation}: the process of adding methyl groups to DNA, in this process modification of covalent nucleotides in the human genome, namely cytosine and also guanine. It is one of epigenetic modification which takes an important role in the development of cancer.
\item \textbf{Gleason Score}: a measure (from 2-10) of the aggression of prostate cancer cells based on clinical pathology of prostate tissue \cite{prostate}.
\item \textbf{High-Grade Prostate Cancer (HGPCa)}: A Gleason score from 8-10 is indicative of High-Grade prostate cancer, while scores below 8 are considered not as severe, HGPCa generally results in poor patient outcomes mortality, complications, and long-term disease-free survival) \cite{prostate}.
\item \textbf{Cancer Gene Catalogues}: a catalogue those genes which contain mutations that have been causally implicated in cancer.
\end{itemize}

\subsection{Dataset}
The dataset come from the National Cancer Institute GDC Data Portal and can be found online under Project TCGA-PRAD (The Cancer Genome Atlas Prostate Adenocarcinoma) \cite{praddata}. This dataset consists of genomic information and clinical pathology reports belonging to 549 patients who have been diagnosed with prostate cancer (a Gleason Score anywhere between 2 to 10). It contains both clinical (recurrence, survival \& treatment resistance) and molecular profiles (Exon (mRNA) expression, DNA methylation, Copy Number Variations (CNV) \& Single Nucleotide Polymorphism (SNP)) for both tumor samples and normal controls. The file that we used are:
\begin{itemize}
\item \textbf{PRAD.meth.by\_mean.data}: the mean signal values among each gene corresponding to the patient barcode. The size is 20111 genes x 549 patients (52 MB). This data become our learning features.
\item \textbf{All\_CDEs}: clinical data elements corresponding to the patient barcode. It contains many related information, but we only consider the gleason score.
\end{itemize}

\subsection{System Design}
The process consists of preprocessing of TCGA-PRAD dataset, consist of feature selection, missing data imputation \& adding label. After that, split the dataset into training \& testing data, next we training \& evaluating the classifier models. The system design can be seen in Figure \ref{fig:system_design}. The system specification where this project running on:
\begin{itemize}
\item \textbf{Programming language}: Python
\item \textbf{Support library}: Scikit-Learn \& Pandas
\item \textbf{PC}: Windows 10 64-bit, Intel i5-7500, RAM 4 GB
\end{itemize}

\begin{figure}
  \includegraphics[width=1\linewidth]{system_design}
  \centering
  \caption{System Design}
  \label{fig:system_design}
\end{figure}

\subsection{Preprocessing}
\begin{itemize}
\item \textbf{Feature selection}: To reduce the feature space of the dataset, only those genes for which mutations have been causally implicated in cancer are considered, these are obtained through resources like \textit{COSMIC} (Catalogue of Somatic Mutations in Cancer) that consist of 3946 genes information \cite{cosmicdata}, and \textit{CIVIC} (Clinical Interpretation of Variants in Cancer) that consist of 3946 genes information \cite{civicdata}. Originally, our dataset has 20111 genes, after feature selection it has only 821 genes.
\item \textbf{Missing data imputation}: It is the process of replacing missing data of dataset features with substituted values. This project using mean imputation.
\item \textbf{Adding label}: We categorized into 2 classes, between score of 2 to 7 (not severe) and 8 to 10 (HGPCa / severe)  from All\_CDEs data. We added class labels to the dataset with correspond to the patient id.

\end{itemize}

\subsection{Classification}
\begin{itemize}
\item \textbf{Dataset split}: We split the dataset, 70\% for train data \& 30\% for test data of each data group (not severe and severe/HGPCa that illustrated in Figure \ref{fig:data_split}.
\begin{figure}
  \includegraphics[width=0.4\linewidth]{data_split}
  \centering
  \caption{Dataset split}
  \label{fig:data_split}
\end{figure}
\item \textbf{Training and evaluation}: This project perform on several machine learning algorithms, namely \textit{Logistic Regression} (LR), \textit{K-Nearest Neighbor} (KNN), \textit{Support Vector Machine} (SVM), \textit{Multilayer Perceptron} (MLP). In the experiments, we evaluate parameters for each machine learning method to find the one that gives highest accuracy.
\end{itemize}

\section{Experiments}
\begin{itemize}
\item \textbf{Evaluation on Logistic Regression} : Logistic regression is a technique that finds the optimal linear boundary between the two classes. In order to combat overfitting, it is common practice to add a regularization term logistic models. With logistic regression we have the option between the L1 and L2 norm. Although the only difference between these two terms is that one is simply a sum (L1) and the other is simply a square of sums (L2). The result are\\

\item \textbf{Evaluation on K-Nearest Neighbor} : Logistic regression is a technique that finds the optimal linear boundary between the two classes. In order to combat overfitting, it is common practice to add a regularization term logistic models. With logistic regression we have the option between the L1 and L2 norm. Although the only difference between these two terms is that one is simply a sum (L1) and the other is simply a square of sums (L2). The result are\\

\item \textbf{Evaluation on Support Vector Machine} : Logistic regression is a technique that finds the optimal linear boundary between the two classes. In order to combat overfitting, it is common practice to add a regularization term logistic models. With logistic regression we have the option between the L1 and L2 norm. Although the only difference between these two terms is that one is simply a sum (L1) and the other is simply a square of sums (L2). The result are\\

\item \textbf{Evaluation on Multilayer Perceptron} : Logistic regression is a technique that finds the optimal linear boundary between the two classes. In order to combat overfitting, it is common practice to add a regularization term logistic models. With logistic regression we have the option between the L1 and L2 norm. Although the only difference between these two terms is that one is simply a sum (L1) and the other is simply a square of sums (L2). The result are
\end{itemize}

\begin{figure}
	\centering
	\begin{minipage}[t]{8.6cm}
\includegraphics[width=1\linewidth]{LR}
  \centering
  \caption{Evaluation on Logistic Regression}
  \label{fig:LR}
	\end{minipage}
	\begin{minipage}[t]{8.6cm}
	\includegraphics[width=1\linewidth]{KNN}
  \centering
  \caption{Evaluation on K-Nearest Neighbor}
  \label{fig:KNN}
	\end{minipage}
\end{figure}

\begin{figure}
	\centering
	\begin{minipage}[t]{8.6cm}
\includegraphics[width=1\linewidth]{SVM}
  \centering
  \caption{Evaluation on Support Vector Machine}
  \label{fig:SVM}
	\end{minipage}
	\begin{minipage}[t]{8.6cm}
	\includegraphics[width=1\linewidth]{MLP}
  \centering
  \caption{Evaluation on Multilayer Perceptron}
  \label{fig:MLP}
	\end{minipage}
\end{figure}


\section{Conclusion}
\begin{itemize}
\item \textbf{Result}: 
\item \textbf{Future work}: 
\end{itemize}

\begin{thebibliography}{1}
\bibitem{prostate} Ilir Agalliu, Robert Gern, Suzanne Leanza, and Robert D. Burk. 2009. {\em Associations of High-Grade Prostate Cancer with BRCA1 and BRCA2 Founder Mutations.} Clinical Cancer Research. Vol. 15, No. 3.
\bibitem{dnameth} Hao, Xiaoke, et al. {\em DNA methylation markers for diagnosis and prognosis of common cancers.} Proceedings of the National Academy of Sciences 114.28 (2017): 7414-7419.
\bibitem{praddata} Genome.ifmo.ru. (2018). TCGA PRAD Dataset. [online] Available at: {\em https://genome.ifmo.ru/files/software/phantasus/tcga/PRAD/ } [Accessed 29 Nov. 2018].
\bibitem{cosmicdata} Cancer.sanger.ac.uk. (2018). Cancer Gene Census. [online] Available at: {\em https://cancer.sanger.ac.uk/census } [Accessed 29 Nov. 2018].
\bibitem{civicdata} Civicdb.org. (2018). CIViC - Clinical Interpretations of Variants in Cancer. [online] Available at: {\em https://civicdb.org/releases } [Accessed 29 Nov. 2018].
\end{thebibliography}
\end{document}