> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 0.7454545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> Applying KNN
> n=2
> Accuracy = 0.6424242424242425
              precision    recall  f1-score   support

         0.0       0.63      0.98      0.77       101
         1.0       0.78      0.11      0.19        64

   micro avg       0.64      0.64      0.64       165
   macro avg       0.71      0.54      0.48       165
weighted avg       0.69      0.64      0.55       165

> n=3
> Accuracy = 0.6848484848484848
              precision    recall  f1-score   support

         0.0       0.68      0.92      0.78       101
         1.0       0.71      0.31      0.43        64

   micro avg       0.68      0.68      0.68       165
   macro avg       0.70      0.62      0.61       165
weighted avg       0.69      0.68      0.65       165

> n=4
> Accuracy = 0.6606060606060606
              precision    recall  f1-score   support

         0.0       0.65      0.98      0.78       101
         1.0       0.83      0.16      0.26        64

   micro avg       0.66      0.66      0.66       165
   macro avg       0.74      0.57      0.52       165
weighted avg       0.72      0.66      0.58       165

> n=5
> Accuracy = 0.6484848484848484
              precision    recall  f1-score   support

         0.0       0.65      0.90      0.76       101
         1.0       0.62      0.25      0.36        64

   micro avg       0.65      0.65      0.65       165
   macro avg       0.64      0.58      0.56       165
weighted avg       0.64      0.65      0.60       165

> Applying SVM
> linear
> Accuracy = 0.7696969696969697
              precision    recall  f1-score   support

         0.0       0.75      0.93      0.83       101
         1.0       0.82      0.52      0.63        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.79      0.72      0.73       165
weighted avg       0.78      0.77      0.76       165

> rbf
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 2
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 3
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 4
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> Applying MLP
> sigmoid
1 layer
> Accuracy = 0.7212121212121212
              precision    recall  f1-score   support

         0.0       0.71      0.93      0.80       101
         1.0       0.78      0.39      0.52        64

   micro avg       0.72      0.72      0.72       165
   macro avg       0.74      0.66      0.66       165
weighted avg       0.74      0.72      0.69       165

> sigmoid
2 layer
> Accuracy = 0.7515151515151515
              precision    recall  f1-score   support

         0.0       0.73      0.94      0.82       101
         1.0       0.83      0.45      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.78      0.70      0.70       165
weighted avg       0.77      0.75      0.73       165

> sigmoid
3 layer
> Accuracy = 0.696969696969697
              precision    recall  f1-score   support

         0.0       0.68      0.95      0.79       101
         1.0       0.79      0.30      0.43        64

   micro avg       0.70      0.70      0.70       165
   macro avg       0.74      0.62      0.61       165
weighted avg       0.72      0.70      0.65       165

> relu
1 layer
> Accuracy = 0.7090909090909091
              precision    recall  f1-score   support

         0.0       0.69      0.96      0.80       101
         1.0       0.83      0.31      0.45        64

   micro avg       0.71      0.71      0.71       165
   macro avg       0.76      0.64      0.63       165
weighted avg       0.74      0.71      0.67       165

> relu
2 layer
> Accuracy = 0.7333333333333333
              precision    recall  f1-score   support

         0.0       0.74      0.87      0.80       101
         1.0       0.72      0.52      0.60        64

   micro avg       0.73      0.73      0.73       165
   macro avg       0.73      0.69      0.70       165
weighted avg       0.73      0.73      0.72       165

> relu
3 layer
> Accuracy = 0.7515151515151515
              precision    recall  f1-score   support

         0.0       0.80      0.79      0.80       101
         1.0       0.68      0.69      0.68        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.74      0.74      0.74       165
weighted avg       0.75      0.75      0.75       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 0.7454545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 0.7454545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 0.7454545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> Applying KNN
> n=2
> Accuracy = 0.6424242424242425
              precision    recall  f1-score   support

         0.0       0.63      0.98      0.77       101
         1.0       0.78      0.11      0.19        64

   micro avg       0.64      0.64      0.64       165
   macro avg       0.71      0.54      0.48       165
weighted avg       0.69      0.64      0.55       165

> n=3
> Accuracy = 0.6848484848484848
              precision    recall  f1-score   support

         0.0       0.68      0.92      0.78       101
         1.0       0.71      0.31      0.43        64

   micro avg       0.68      0.68      0.68       165
   macro avg       0.70      0.62      0.61       165
weighted avg       0.69      0.68      0.65       165

> n=4
> Accuracy = 0.6606060606060606
              precision    recall  f1-score   support

         0.0       0.65      0.98      0.78       101
         1.0       0.83      0.16      0.26        64

   micro avg       0.66      0.66      0.66       165
   macro avg       0.74      0.57      0.52       165
weighted avg       0.72      0.66      0.58       165

> n=5
> Accuracy = 0.6484848484848484
              precision    recall  f1-score   support

         0.0       0.65      0.90      0.76       101
         1.0       0.62      0.25      0.36        64

   micro avg       0.65      0.65      0.65       165
   macro avg       0.64      0.58      0.56       165
weighted avg       0.64      0.65      0.60       165

> Applying SVM
> linear
> Accuracy = 0.7696969696969697
              precision    recall  f1-score   support

         0.0       0.75      0.93      0.83       101
         1.0       0.82      0.52      0.63        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.79      0.72      0.73       165
weighted avg       0.78      0.77      0.76       165

> rbf
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 2
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 3
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 4
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> Applying MLP
> sigmoid
1 layer
> Accuracy = 0.7212121212121212
              precision    recall  f1-score   support

         0.0       0.70      0.94      0.81       101
         1.0       0.80      0.38      0.51        64

   micro avg       0.72      0.72      0.72       165
   macro avg       0.75      0.66      0.66       165
weighted avg       0.74      0.72      0.69       165

> sigmoid
2 layer
> Accuracy = 0.7696969696969697
              precision    recall  f1-score   support

         0.0       0.75      0.93      0.83       101
         1.0       0.82      0.52      0.63        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.79      0.72      0.73       165
weighted avg       0.78      0.77      0.76       165

> sigmoid
3 layer
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> relu
1 layer
> Accuracy = 0.7272727272727273
              precision    recall  f1-score   support

         0.0       0.72      0.91      0.80       101
         1.0       0.76      0.44      0.55        64

   micro avg       0.73      0.73      0.73       165
   macro avg       0.74      0.67      0.68       165
weighted avg       0.73      0.73      0.71       165

> relu
2 layer
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.75      0.92      0.83       101
         1.0       0.80      0.52      0.63        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.78      0.72      0.73       165
weighted avg       0.77      0.76      0.75       165

> relu
3 layer
> Accuracy = 0.703030303030303
              precision    recall  f1-score   support

         0.0       0.69      0.92      0.79       101
         1.0       0.74      0.36      0.48        64

   micro avg       0.70      0.70      0.70       165
   macro avg       0.72      0.64      0.64       165
weighted avg       0.71      0.70      0.67       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 0.7454545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> Applying KNN
> n=2
> Accuracy = 0.6424242424242425
              precision    recall  f1-score   support

         0.0       0.63      0.98      0.77       101
         1.0       0.78      0.11      0.19        64

   micro avg       0.64      0.64      0.64       165
   macro avg       0.71      0.54      0.48       165
weighted avg       0.69      0.64      0.55       165

> n=3
> Accuracy = 0.6848484848484848
              precision    recall  f1-score   support

         0.0       0.68      0.92      0.78       101
         1.0       0.71      0.31      0.43        64

   micro avg       0.68      0.68      0.68       165
   macro avg       0.70      0.62      0.61       165
weighted avg       0.69      0.68      0.65       165

> n=4
> Accuracy = 0.6606060606060606
              precision    recall  f1-score   support

         0.0       0.65      0.98      0.78       101
         1.0       0.83      0.16      0.26        64

   micro avg       0.66      0.66      0.66       165
   macro avg       0.74      0.57      0.52       165
weighted avg       0.72      0.66      0.58       165

> n=5
> Accuracy = 0.6484848484848484
              precision    recall  f1-score   support

         0.0       0.65      0.90      0.76       101
         1.0       0.62      0.25      0.36        64

   micro avg       0.65      0.65      0.65       165
   macro avg       0.64      0.58      0.56       165
weighted avg       0.64      0.65      0.60       165

> Applying SVM
> linear
> Accuracy = 0.7696969696969697
              precision    recall  f1-score   support

         0.0       0.75      0.93      0.83       101
         1.0       0.82      0.52      0.63        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.79      0.72      0.73       165
weighted avg       0.78      0.77      0.76       165

> rbf
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 2
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 3
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 4
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> Applying MLP
> sigmoid
1 layer
> Accuracy = 0.7090909090909091
              precision    recall  f1-score   support

         0.0       0.69      0.94      0.80       101
         1.0       0.79      0.34      0.48        64

   micro avg       0.71      0.71      0.71       165
   macro avg       0.74      0.64      0.64       165
weighted avg       0.73      0.71      0.67       165

> sigmoid
2 layer
> Accuracy = 0.7575757575757576
              precision    recall  f1-score   support

         0.0       0.74      0.93      0.82       101
         1.0       0.82      0.48      0.61        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.78      0.71      0.72       165
weighted avg       0.77      0.76      0.74       165

> sigmoid
3 layer
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> relu
1 layer
> Accuracy = 0.7818181818181819
              precision    recall  f1-score   support

         0.0       0.78      0.89      0.83       101
         1.0       0.78      0.61      0.68        64

   micro avg       0.78      0.78      0.78       165
   macro avg       0.78      0.75      0.76       165
weighted avg       0.78      0.78      0.78       165

> relu
2 layer
> Accuracy = 0.7454545454545455
              precision    recall  f1-score   support

         0.0       0.85      0.71      0.77       101
         1.0       0.64      0.80      0.71        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.74      0.75      0.74       165
weighted avg       0.77      0.75      0.75       165

> relu
3 layer
> Accuracy = 0.6545454545454545
              precision    recall  f1-score   support

         0.0       0.64      0.99      0.78       101
         1.0       0.89      0.12      0.22        64

   micro avg       0.65      0.65      0.65       165
   macro avg       0.76      0.56      0.50       165
weighted avg       0.74      0.65      0.56       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 0.7454545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 0.7454545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 0.7575757575757576
              precision    recall  f1-score   support

         0.0       0.72      0.98      0.83       101
         1.0       0.93      0.41      0.57        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.69      0.70       165
weighted avg       0.80      0.76      0.73       165

> l2 penalty
> Accuracy = 0.7454545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 0.7454545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 0.7454545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 0.7454545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 0.7454545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 0.7454545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 0.7454545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> Applying KNN
> n=2
> Accuracy = 0.6424242424242425
              precision    recall  f1-score   support

         0.0       0.63      0.98      0.77       101
         1.0       0.78      0.11      0.19        64

   micro avg       0.64      0.64      0.64       165
   macro avg       0.71      0.54      0.48       165
weighted avg       0.69      0.64      0.55       165

> n=3
> Accuracy = 0.6848484848484848
              precision    recall  f1-score   support

         0.0       0.68      0.92      0.78       101
         1.0       0.71      0.31      0.43        64

   micro avg       0.68      0.68      0.68       165
   macro avg       0.70      0.62      0.61       165
weighted avg       0.69      0.68      0.65       165

> n=4
> Accuracy = 0.6606060606060606
              precision    recall  f1-score   support

         0.0       0.65      0.98      0.78       101
         1.0       0.83      0.16      0.26        64

   micro avg       0.66      0.66      0.66       165
   macro avg       0.74      0.57      0.52       165
weighted avg       0.72      0.66      0.58       165

> n=5
> Accuracy = 0.6484848484848484
              precision    recall  f1-score   support

         0.0       0.65      0.90      0.76       101
         1.0       0.62      0.25      0.36        64

   micro avg       0.65      0.65      0.65       165
   macro avg       0.64      0.58      0.56       165
weighted avg       0.64      0.65      0.60       165

> Applying SVM
> linear
> Accuracy = 0.7696969696969697
              precision    recall  f1-score   support

         0.0       0.75      0.93      0.83       101
         1.0       0.82      0.52      0.63        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.79      0.72      0.73       165
weighted avg       0.78      0.77      0.76       165

> rbf
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 2
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 3
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 4
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> Applying MLP
> sigmoid
1 layer
> Accuracy = 0.7272727272727273
              precision    recall  f1-score   support

         0.0       0.71      0.94      0.81       101
         1.0       0.81      0.39      0.53        64

   micro avg       0.73      0.73      0.73       165
   macro avg       0.76      0.67      0.67       165
weighted avg       0.75      0.73      0.70       165

> sigmoid
2 layer
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> sigmoid
3 layer
> Accuracy = 0.7515151515151515
              precision    recall  f1-score   support

         0.0       0.74      0.91      0.82       101
         1.0       0.78      0.50      0.61        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.71      0.71       165
weighted avg       0.76      0.75      0.74       165

> relu
1 layer
> Accuracy = 0.7212121212121212
              precision    recall  f1-score   support

         0.0       0.70      0.96      0.81       101
         1.0       0.85      0.34      0.49        64

   micro avg       0.72      0.72      0.72       165
   macro avg       0.77      0.65      0.65       165
weighted avg       0.76      0.72      0.68       165

> relu
2 layer
> Accuracy = 0.7757575757575758
              precision    recall  f1-score   support

         0.0       0.80      0.85      0.82       101
         1.0       0.74      0.66      0.69        64

   micro avg       0.78      0.78      0.78       165
   macro avg       0.77      0.75      0.76       165
weighted avg       0.77      0.78      0.77       165

> relu
3 layer
> Accuracy = 0.7575757575757576
              precision    recall  f1-score   support

         0.0       0.81      0.78      0.80       101
         1.0       0.68      0.72      0.70        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.75      0.75      0.75       165
weighted avg       0.76      0.76      0.76       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 0.7454545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> Applying KNN
> n=2
> Accuracy = 0.6424242424242425
              precision    recall  f1-score   support

         0.0       0.63      0.98      0.77       101
         1.0       0.78      0.11      0.19        64

   micro avg       0.64      0.64      0.64       165
   macro avg       0.71      0.54      0.48       165
weighted avg       0.69      0.64      0.55       165

> n=3
> Accuracy = 0.6848484848484848
              precision    recall  f1-score   support

         0.0       0.68      0.92      0.78       101
         1.0       0.71      0.31      0.43        64

   micro avg       0.68      0.68      0.68       165
   macro avg       0.70      0.62      0.61       165
weighted avg       0.69      0.68      0.65       165

> n=4
> Accuracy = 0.6606060606060606
              precision    recall  f1-score   support

         0.0       0.65      0.98      0.78       101
         1.0       0.83      0.16      0.26        64

   micro avg       0.66      0.66      0.66       165
   macro avg       0.74      0.57      0.52       165
weighted avg       0.72      0.66      0.58       165

> n=5
> Accuracy = 0.6484848484848484
              precision    recall  f1-score   support

         0.0       0.65      0.90      0.76       101
         1.0       0.62      0.25      0.36        64

   micro avg       0.65      0.65      0.65       165
   macro avg       0.64      0.58      0.56       165
weighted avg       0.64      0.65      0.60       165

> Applying SVM
> linear
> Accuracy = 0.7696969696969697
              precision    recall  f1-score   support

         0.0       0.75      0.93      0.83       101
         1.0       0.82      0.52      0.63        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.79      0.72      0.73       165
weighted avg       0.78      0.77      0.76       165

> rbf
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 2
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 3
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 4
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> Applying MLP
> sigmoid
1 layer
> Accuracy = 0.7151515151515152
              precision    recall  f1-score   support

         0.0       0.70      0.93      0.80       101
         1.0       0.77      0.38      0.51        64

   micro avg       0.72      0.72      0.72       165
   macro avg       0.74      0.65      0.65       165
weighted avg       0.73      0.72      0.69       165

> sigmoid
2 layer
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.80      0.82      0.81       101
         1.0       0.70      0.67      0.69        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.75      0.75      0.75       165
weighted avg       0.76      0.76      0.76       165

> sigmoid
3 layer
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> relu
1 layer
> Accuracy = 0.7151515151515152
              precision    recall  f1-score   support

         0.0       0.70      0.95      0.80       101
         1.0       0.81      0.34      0.48        64

   micro avg       0.72      0.72      0.72       165
   macro avg       0.76      0.65      0.64       165
weighted avg       0.74      0.72      0.68       165

> relu
2 layer
> Accuracy = 0.7090909090909091
              precision    recall  f1-score   support

         0.0       0.69      0.97      0.80       101
         1.0       0.86      0.30      0.44        64

   micro avg       0.71      0.71      0.71       165
   macro avg       0.77      0.63      0.62       165
weighted avg       0.75      0.71      0.66       165

> relu
3 layer
> Accuracy = 0.7696969696969697
              precision    recall  f1-score   support

         0.0       0.78      0.86      0.82       101
         1.0       0.74      0.62      0.68        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.76      0.74      0.75       165
weighted avg       0.77      0.77      0.77       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 0.7454545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> Applying KNN
> n=2
> Accuracy = 0.6424242424242425
              precision    recall  f1-score   support

         0.0       0.63      0.98      0.77       101
         1.0       0.78      0.11      0.19        64

   micro avg       0.64      0.64      0.64       165
   macro avg       0.71      0.54      0.48       165
weighted avg       0.69      0.64      0.55       165

> n=3
> Accuracy = 0.6848484848484848
              precision    recall  f1-score   support

         0.0       0.68      0.92      0.78       101
         1.0       0.71      0.31      0.43        64

   micro avg       0.68      0.68      0.68       165
   macro avg       0.70      0.62      0.61       165
weighted avg       0.69      0.68      0.65       165

> n=4
> Accuracy = 0.6606060606060606
              precision    recall  f1-score   support

         0.0       0.65      0.98      0.78       101
         1.0       0.83      0.16      0.26        64

   micro avg       0.66      0.66      0.66       165
   macro avg       0.74      0.57      0.52       165
weighted avg       0.72      0.66      0.58       165

> n=5
> Accuracy = 0.6484848484848484
              precision    recall  f1-score   support

         0.0       0.65      0.90      0.76       101
         1.0       0.62      0.25      0.36        64

   micro avg       0.65      0.65      0.65       165
   macro avg       0.64      0.58      0.56       165
weighted avg       0.64      0.65      0.60       165

> Applying SVM
> linear
> Accuracy = 0.7696969696969697
              precision    recall  f1-score   support

         0.0       0.75      0.93      0.83       101
         1.0       0.82      0.52      0.63        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.79      0.72      0.73       165
weighted avg       0.78      0.77      0.76       165

> rbf
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 2
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 3
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 4
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> Applying MLP
> sigmoid
1 layer
> Accuracy = 0.7696969696969697
              precision    recall  f1-score   support

         0.0       0.76      0.90      0.83       101
         1.0       0.78      0.56      0.65        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.77      0.73      0.74       165
weighted avg       0.77      0.77      0.76       165

> sigmoid
2 layer
> Accuracy = 0.703030303030303
              precision    recall  f1-score   support

         0.0       0.68      0.96      0.80       101
         1.0       0.83      0.30      0.44        64

   micro avg       0.70      0.70      0.70       165
   macro avg       0.75      0.63      0.62       165
weighted avg       0.74      0.70      0.66       165

> sigmoid
3 layer
> Accuracy = 0.7696969696969697
              precision    recall  f1-score   support

         0.0       0.79      0.85      0.82       101
         1.0       0.73      0.64      0.68        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.76      0.75      0.75       165
weighted avg       0.77      0.77      0.77       165

> relu
1 layer
> Accuracy = 0.7090909090909091
              precision    recall  f1-score   support

         0.0       0.69      0.97      0.80       101
         1.0       0.86      0.30      0.44        64

   micro avg       0.71      0.71      0.71       165
   macro avg       0.77      0.63      0.62       165
weighted avg       0.75      0.71      0.66       165

> relu
2 layer
> Accuracy = 0.7151515151515152
              precision    recall  f1-score   support

         0.0       0.69      0.96      0.80       101
         1.0       0.84      0.33      0.47        64

   micro avg       0.72      0.72      0.72       165
   macro avg       0.77      0.64      0.64       165
weighted avg       0.75      0.72      0.68       165

> relu
3 layer
> Accuracy = 0.6909090909090909
              precision    recall  f1-score   support

         0.0       0.67      0.96      0.79       101
         1.0       0.81      0.27      0.40        64

   micro avg       0.69      0.69      0.69       165
   macro avg       0.74      0.61      0.60       165
weighted avg       0.73      0.69      0.64       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 0.7454545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> Applying KNN
> n=2
> Accuracy = 0.6424242424242425
              precision    recall  f1-score   support

         0.0       0.63      0.98      0.77       101
         1.0       0.78      0.11      0.19        64

   micro avg       0.64      0.64      0.64       165
   macro avg       0.71      0.54      0.48       165
weighted avg       0.69      0.64      0.55       165

> n=3
> Accuracy = 0.6848484848484848
              precision    recall  f1-score   support

         0.0       0.68      0.92      0.78       101
         1.0       0.71      0.31      0.43        64

   micro avg       0.68      0.68      0.68       165
   macro avg       0.70      0.62      0.61       165
weighted avg       0.69      0.68      0.65       165

> n=4
> Accuracy = 0.6606060606060606
              precision    recall  f1-score   support

         0.0       0.65      0.98      0.78       101
         1.0       0.83      0.16      0.26        64

   micro avg       0.66      0.66      0.66       165
   macro avg       0.74      0.57      0.52       165
weighted avg       0.72      0.66      0.58       165

> n=5
> Accuracy = 0.6484848484848484
              precision    recall  f1-score   support

         0.0       0.65      0.90      0.76       101
         1.0       0.62      0.25      0.36        64

   micro avg       0.65      0.65      0.65       165
   macro avg       0.64      0.58      0.56       165
weighted avg       0.64      0.65      0.60       165

> Applying SVM
> linear
> Accuracy = 0.7696969696969697
              precision    recall  f1-score   support

         0.0       0.75      0.93      0.83       101
         1.0       0.82      0.52      0.63        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.79      0.72      0.73       165
weighted avg       0.78      0.77      0.76       165

> rbf
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 2
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 3
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 4
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> Applying MLP
> sigmoid
1 layer
> Accuracy = 0.7151515151515152
              precision    recall  f1-score   support

         0.0       0.71      0.91      0.80       101
         1.0       0.74      0.41      0.53        64

   micro avg       0.72      0.72      0.72       165
   macro avg       0.73      0.66      0.66       165
weighted avg       0.72      0.72      0.69       165

> sigmoid
2 layer
> Accuracy = 0.7333333333333333
              precision    recall  f1-score   support

         0.0       0.72      0.93      0.81       101
         1.0       0.79      0.42      0.55        64

   micro avg       0.73      0.73      0.73       165
   macro avg       0.76      0.68      0.68       165
weighted avg       0.75      0.73      0.71       165

> sigmoid
3 layer
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> relu
1 layer
> Accuracy = 0.7212121212121212
              precision    recall  f1-score   support

         0.0       0.70      0.95      0.81       101
         1.0       0.82      0.36      0.50        64

   micro avg       0.72      0.72      0.72       165
   macro avg       0.76      0.65      0.65       165
weighted avg       0.75      0.72      0.69       165

> relu
2 layer
> Accuracy = 0.7575757575757576
              precision    recall  f1-score   support

         0.0       0.81      0.79      0.80       101
         1.0       0.68      0.70      0.69        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.74      0.75      0.75       165
weighted avg       0.76      0.76      0.76       165

> relu
3 layer
> Accuracy = 0.7090909090909091
              precision    recall  f1-score   support

         0.0       0.69      0.96      0.80       101
         1.0       0.83      0.31      0.45        64

   micro avg       0.71      0.71      0.71       165
   macro avg       0.76      0.64      0.63       165
weighted avg       0.74      0.71      0.67       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 0.7454545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> Applying KNN
> n=2
> Accuracy = 0.6424242424242425
              precision    recall  f1-score   support

         0.0       0.63      0.98      0.77       101
         1.0       0.78      0.11      0.19        64

   micro avg       0.64      0.64      0.64       165
   macro avg       0.71      0.54      0.48       165
weighted avg       0.69      0.64      0.55       165

> n=3
> Accuracy = 0.6848484848484848
              precision    recall  f1-score   support

         0.0       0.68      0.92      0.78       101
         1.0       0.71      0.31      0.43        64

   micro avg       0.68      0.68      0.68       165
   macro avg       0.70      0.62      0.61       165
weighted avg       0.69      0.68      0.65       165

> n=4
> Accuracy = 0.6606060606060606
              precision    recall  f1-score   support

         0.0       0.65      0.98      0.78       101
         1.0       0.83      0.16      0.26        64

   micro avg       0.66      0.66      0.66       165
   macro avg       0.74      0.57      0.52       165
weighted avg       0.72      0.66      0.58       165

> n=5
> Accuracy = 0.6484848484848484
              precision    recall  f1-score   support

         0.0       0.65      0.90      0.76       101
         1.0       0.62      0.25      0.36        64

   micro avg       0.65      0.65      0.65       165
   macro avg       0.64      0.58      0.56       165
weighted avg       0.64      0.65      0.60       165

> Applying SVM
> linear
> Accuracy = 0.7696969696969697
              precision    recall  f1-score   support

         0.0       0.75      0.93      0.83       101
         1.0       0.82      0.52      0.63        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.79      0.72      0.73       165
weighted avg       0.78      0.77      0.76       165

> rbf
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 2
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 3
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 4
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> Applying MLP
> sigmoid
1 layer
> Accuracy = 0.7212121212121212
              precision    recall  f1-score   support

         0.0       0.71      0.91      0.80       101
         1.0       0.75      0.42      0.54        64

   micro avg       0.72      0.72      0.72       165
   macro avg       0.73      0.67      0.67       165
weighted avg       0.73      0.72      0.70       165

> sigmoid
2 layer
> Accuracy = 0.7696969696969697
              precision    recall  f1-score   support

         0.0       0.75      0.93      0.83       101
         1.0       0.82      0.52      0.63        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.79      0.72      0.73       165
weighted avg       0.78      0.77      0.76       165

> sigmoid
3 layer
> Accuracy = 0.7757575757575758
              precision    recall  f1-score   support

         0.0       0.78      0.89      0.83       101
         1.0       0.78      0.59      0.67        64

   micro avg       0.78      0.78      0.78       165
   macro avg       0.78      0.74      0.75       165
weighted avg       0.78      0.78      0.77       165

> relu
1 layer
> Accuracy = 0.696969696969697
              precision    recall  f1-score   support

         0.0       0.68      0.96      0.80       101
         1.0       0.82      0.28      0.42        64

   micro avg       0.70      0.70      0.70       165
   macro avg       0.75      0.62      0.61       165
weighted avg       0.73      0.70      0.65       165

> relu
2 layer
> Accuracy = 0.7454545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> relu
3 layer
> Accuracy = 0.7090909090909091
              precision    recall  f1-score   support

         0.0       0.69      0.96      0.80       101
         1.0       0.83      0.31      0.45        64

   micro avg       0.71      0.71      0.71       165
   macro avg       0.76      0.64      0.63       165
weighted avg       0.74      0.71      0.67       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 0.7454545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> Applying KNN
> n=2
> Accuracy = 0.6424242424242425
              precision    recall  f1-score   support

         0.0       0.63      0.98      0.77       101
         1.0       0.78      0.11      0.19        64

   micro avg       0.64      0.64      0.64       165
   macro avg       0.71      0.54      0.48       165
weighted avg       0.69      0.64      0.55       165

> n=3
> Accuracy = 0.6848484848484848
              precision    recall  f1-score   support

         0.0       0.68      0.92      0.78       101
         1.0       0.71      0.31      0.43        64

   micro avg       0.68      0.68      0.68       165
   macro avg       0.70      0.62      0.61       165
weighted avg       0.69      0.68      0.65       165

> n=4
> Accuracy = 0.6606060606060606
              precision    recall  f1-score   support

         0.0       0.65      0.98      0.78       101
         1.0       0.83      0.16      0.26        64

   micro avg       0.66      0.66      0.66       165
   macro avg       0.74      0.57      0.52       165
weighted avg       0.72      0.66      0.58       165

> n=5
> Accuracy = 0.6484848484848484
              precision    recall  f1-score   support

         0.0       0.65      0.90      0.76       101
         1.0       0.62      0.25      0.36        64

   micro avg       0.65      0.65      0.65       165
   macro avg       0.64      0.58      0.56       165
weighted avg       0.64      0.65      0.60       165

> Applying SVM
> linear
> Accuracy = 0.7696969696969697
              precision    recall  f1-score   support

         0.0       0.75      0.93      0.83       101
         1.0       0.82      0.52      0.63        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.79      0.72      0.73       165
weighted avg       0.78      0.77      0.76       165

> rbf
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 2
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 3
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 4
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> Applying MLP
> sigmoid
1 layer
> Accuracy = 0.7151515151515152
              precision    recall  f1-score   support

         0.0       0.70      0.95      0.80       101
         1.0       0.81      0.34      0.48        64

   micro avg       0.72      0.72      0.72       165
   macro avg       0.76      0.65      0.64       165
weighted avg       0.74      0.72      0.68       165

> sigmoid
2 layer
> Accuracy = 0.7696969696969697
              precision    recall  f1-score   support

         0.0       0.79      0.85      0.82       101
         1.0       0.73      0.64      0.68        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.76      0.75      0.75       165
weighted avg       0.77      0.77      0.77       165

> sigmoid
3 layer
> Accuracy = 0.7212121212121212
              precision    recall  f1-score   support

         0.0       0.70      0.95      0.81       101
         1.0       0.82      0.36      0.50        64

   micro avg       0.72      0.72      0.72       165
   macro avg       0.76      0.65      0.65       165
weighted avg       0.75      0.72      0.69       165

> relu
1 layer
> Accuracy = 0.7696969696969697
              precision    recall  f1-score   support

         0.0       0.80      0.83      0.82       101
         1.0       0.72      0.67      0.69        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.76      0.75      0.75       165
weighted avg       0.77      0.77      0.77       165

> relu
2 layer
> Accuracy = 0.7575757575757576
              precision    recall  f1-score   support

         0.0       0.84      0.74      0.79       101
         1.0       0.66      0.78      0.71        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.75      0.76      0.75       165
weighted avg       0.77      0.76      0.76       165

> relu
3 layer
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.79      0.84      0.81       101
         1.0       0.72      0.64      0.68        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.75      0.74      0.75       165
weighted avg       0.76      0.76      0.76       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 0.7454545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> Applying KNN
> n=2
> Accuracy = 0.6424242424242425
              precision    recall  f1-score   support

         0.0       0.63      0.98      0.77       101
         1.0       0.78      0.11      0.19        64

   micro avg       0.64      0.64      0.64       165
   macro avg       0.71      0.54      0.48       165
weighted avg       0.69      0.64      0.55       165

> n=3
> Accuracy = 0.6848484848484848
              precision    recall  f1-score   support

         0.0       0.68      0.92      0.78       101
         1.0       0.71      0.31      0.43        64

   micro avg       0.68      0.68      0.68       165
   macro avg       0.70      0.62      0.61       165
weighted avg       0.69      0.68      0.65       165

> n=4
> Accuracy = 0.6606060606060606
              precision    recall  f1-score   support

         0.0       0.65      0.98      0.78       101
         1.0       0.83      0.16      0.26        64

   micro avg       0.66      0.66      0.66       165
   macro avg       0.74      0.57      0.52       165
weighted avg       0.72      0.66      0.58       165

> n=5
> Accuracy = 0.6484848484848484
              precision    recall  f1-score   support

         0.0       0.65      0.90      0.76       101
         1.0       0.62      0.25      0.36        64

   micro avg       0.65      0.65      0.65       165
   macro avg       0.64      0.58      0.56       165
weighted avg       0.64      0.65      0.60       165

> Applying SVM
> linear
> Accuracy = 0.7696969696969697
              precision    recall  f1-score   support

         0.0       0.75      0.93      0.83       101
         1.0       0.82      0.52      0.63        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.79      0.72      0.73       165
weighted avg       0.78      0.77      0.76       165

> rbf
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 2
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 3
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 4
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> Applying MLP
> sigmoid
1 layer
> Accuracy = 0.7212121212121212
              precision    recall  f1-score   support

         0.0       0.71      0.91      0.80       101
         1.0       0.75      0.42      0.54        64

   micro avg       0.72      0.72      0.72       165
   macro avg       0.73      0.67      0.67       165
weighted avg       0.73      0.72      0.70       165

> sigmoid
2 layer
> Accuracy = 0.7575757575757576
              precision    recall  f1-score   support

         0.0       0.74      0.93      0.82       101
         1.0       0.82      0.48      0.61        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.78      0.71      0.72       165
weighted avg       0.77      0.76      0.74       165

> sigmoid
3 layer
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> relu
1 layer
> Accuracy = 0.703030303030303
              precision    recall  f1-score   support

         0.0       0.68      0.96      0.80       101
         1.0       0.83      0.30      0.44        64

   micro avg       0.70      0.70      0.70       165
   macro avg       0.75      0.63      0.62       165
weighted avg       0.74      0.70      0.66       165

> relu
2 layer
> Accuracy = 0.7515151515151515
              precision    recall  f1-score   support

         0.0       0.73      0.94      0.82       101
         1.0       0.83      0.45      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.78      0.70      0.70       165
weighted avg       0.77      0.75      0.73       165

> relu
3 layer
> Accuracy = 0.7212121212121212
              precision    recall  f1-score   support

         0.0       0.88      0.63      0.74       101
         1.0       0.60      0.86      0.71        64

   micro avg       0.72      0.72      0.72       165
   macro avg       0.74      0.75      0.72       165
weighted avg       0.77      0.72      0.72       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 0.7454545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> Applying KNN
> n=2
> Accuracy = 0.6424242424242425
              precision    recall  f1-score   support

         0.0       0.63      0.98      0.77       101
         1.0       0.78      0.11      0.19        64

   micro avg       0.64      0.64      0.64       165
   macro avg       0.71      0.54      0.48       165
weighted avg       0.69      0.64      0.55       165

> n=3
> Accuracy = 0.6848484848484848
              precision    recall  f1-score   support

         0.0       0.68      0.92      0.78       101
         1.0       0.71      0.31      0.43        64

   micro avg       0.68      0.68      0.68       165
   macro avg       0.70      0.62      0.61       165
weighted avg       0.69      0.68      0.65       165

> n=4
> Accuracy = 0.6606060606060606
              precision    recall  f1-score   support

         0.0       0.65      0.98      0.78       101
         1.0       0.83      0.16      0.26        64

   micro avg       0.66      0.66      0.66       165
   macro avg       0.74      0.57      0.52       165
weighted avg       0.72      0.66      0.58       165

> n=5
> Accuracy = 0.6484848484848484
              precision    recall  f1-score   support

         0.0       0.65      0.90      0.76       101
         1.0       0.62      0.25      0.36        64

   micro avg       0.65      0.65      0.65       165
   macro avg       0.64      0.58      0.56       165
weighted avg       0.64      0.65      0.60       165

> Applying SVM
> linear
> Accuracy = 0.7696969696969697
              precision    recall  f1-score   support

         0.0       0.75      0.93      0.83       101
         1.0       0.82      0.52      0.63        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.79      0.72      0.73       165
weighted avg       0.78      0.77      0.76       165

> rbf
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 2
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 3
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 4
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> Applying MLP
> sigmoid
1 layer
> Accuracy = 0.7090909090909091
              precision    recall  f1-score   support

         0.0       0.69      0.95      0.80       101
         1.0       0.81      0.33      0.47        64

   micro avg       0.71      0.71      0.71       165
   macro avg       0.75      0.64      0.63       165
weighted avg       0.74      0.71      0.67       165

> sigmoid
2 layer
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.77      0.88      0.82       101
         1.0       0.76      0.58      0.65        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.76      0.73      0.74       165
weighted avg       0.76      0.76      0.76       165

> sigmoid
3 layer
> Accuracy = 0.7272727272727273
              precision    recall  f1-score   support

         0.0       0.71      0.94      0.81       101
         1.0       0.81      0.39      0.53        64

   micro avg       0.73      0.73      0.73       165
   macro avg       0.76      0.67      0.67       165
weighted avg       0.75      0.73      0.70       165

> relu
1 layer
> Accuracy = 0.7272727272727273
              precision    recall  f1-score   support

         0.0       0.71      0.94      0.81       101
         1.0       0.81      0.39      0.53        64

   micro avg       0.73      0.73      0.73       165
   macro avg       0.76      0.67      0.67       165
weighted avg       0.75      0.73      0.70       165

> relu
2 layer
> Accuracy = 0.7575757575757576
              precision    recall  f1-score   support

         0.0       0.75      0.91      0.82       101
         1.0       0.79      0.52      0.62        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.77      0.71      0.72       165
weighted avg       0.76      0.76      0.74       165

> relu
3 layer
> Accuracy = 0.7575757575757576
              precision    recall  f1-score   support

         0.0       0.78      0.84      0.81       101
         1.0       0.71      0.62      0.67        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.75      0.73      0.74       165
weighted avg       0.75      0.76      0.75       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 0.7454545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> Applying KNN
> n=2
> Accuracy = 0.6424242424242425
              precision    recall  f1-score   support

         0.0       0.63      0.98      0.77       101
         1.0       0.78      0.11      0.19        64

   micro avg       0.64      0.64      0.64       165
   macro avg       0.71      0.54      0.48       165
weighted avg       0.69      0.64      0.55       165

> n=3
> Accuracy = 0.6848484848484848
              precision    recall  f1-score   support

         0.0       0.68      0.92      0.78       101
         1.0       0.71      0.31      0.43        64

   micro avg       0.68      0.68      0.68       165
   macro avg       0.70      0.62      0.61       165
weighted avg       0.69      0.68      0.65       165

> n=4
> Accuracy = 0.6606060606060606
              precision    recall  f1-score   support

         0.0       0.65      0.98      0.78       101
         1.0       0.83      0.16      0.26        64

   micro avg       0.66      0.66      0.66       165
   macro avg       0.74      0.57      0.52       165
weighted avg       0.72      0.66      0.58       165

> n=5
> Accuracy = 0.6484848484848484
              precision    recall  f1-score   support

         0.0       0.65      0.90      0.76       101
         1.0       0.62      0.25      0.36        64

   micro avg       0.65      0.65      0.65       165
   macro avg       0.64      0.58      0.56       165
weighted avg       0.64      0.65      0.60       165

> Applying SVM
> linear
> Accuracy = 0.7696969696969697
              precision    recall  f1-score   support

         0.0       0.75      0.93      0.83       101
         1.0       0.82      0.52      0.63        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.79      0.72      0.73       165
weighted avg       0.78      0.77      0.76       165

> rbf
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 2
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 3
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 4
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> Applying MLP
> sigmoid
1 layer
> Accuracy = 0.7575757575757576
              precision    recall  f1-score   support

         0.0       0.75      0.90      0.82       101
         1.0       0.77      0.53      0.63        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.76      0.72      0.72       165
weighted avg       0.76      0.76      0.75       165

> sigmoid
2 layer
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> sigmoid
3 layer
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> relu
1 layer
> Accuracy = 0.7696969696969697
              precision    recall  f1-score   support

         0.0       0.76      0.90      0.83       101
         1.0       0.78      0.56      0.65        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.77      0.73      0.74       165
weighted avg       0.77      0.77      0.76       165

> relu
2 layer
> Accuracy = 0.6848484848484848
              precision    recall  f1-score   support

         0.0       0.68      0.92      0.78       101
         1.0       0.71      0.31      0.43        64

   micro avg       0.68      0.68      0.68       165
   macro avg       0.70      0.62      0.61       165
weighted avg       0.69      0.68      0.65       165

> relu
3 layer
> Accuracy = 0.7575757575757576
              precision    recall  f1-score   support

         0.0       0.76      0.89      0.82       101
         1.0       0.76      0.55      0.64        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.76      0.72      0.73       165
weighted avg       0.76      0.76      0.75       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 0.7454545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> Applying KNN
> n=2
> Accuracy = 0.6424242424242425
              precision    recall  f1-score   support

         0.0       0.63      0.98      0.77       101
         1.0       0.78      0.11      0.19        64

   micro avg       0.64      0.64      0.64       165
   macro avg       0.71      0.54      0.48       165
weighted avg       0.69      0.64      0.55       165

> n=3
> Accuracy = 0.6848484848484848
              precision    recall  f1-score   support

         0.0       0.68      0.92      0.78       101
         1.0       0.71      0.31      0.43        64

   micro avg       0.68      0.68      0.68       165
   macro avg       0.70      0.62      0.61       165
weighted avg       0.69      0.68      0.65       165

> n=4
> Accuracy = 0.6606060606060606
              precision    recall  f1-score   support

         0.0       0.65      0.98      0.78       101
         1.0       0.83      0.16      0.26        64

   micro avg       0.66      0.66      0.66       165
   macro avg       0.74      0.57      0.52       165
weighted avg       0.72      0.66      0.58       165

> n=5
> Accuracy = 0.6484848484848484
              precision    recall  f1-score   support

         0.0       0.65      0.90      0.76       101
         1.0       0.62      0.25      0.36        64

   micro avg       0.65      0.65      0.65       165
   macro avg       0.64      0.58      0.56       165
weighted avg       0.64      0.65      0.60       165

> Applying SVM
> linear
> Accuracy = 0.7696969696969697
              precision    recall  f1-score   support

         0.0       0.75      0.93      0.83       101
         1.0       0.82      0.52      0.63        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.79      0.72      0.73       165
weighted avg       0.78      0.77      0.76       165

> rbf
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 2
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 3
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 4
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> Applying MLP
> sigmoid
1 layer
> Accuracy = 0.7151515151515152
              precision    recall  f1-score   support

         0.0       0.70      0.92      0.80       101
         1.0       0.76      0.39      0.52        64

   micro avg       0.72      0.72      0.72       165
   macro avg       0.73      0.66      0.66       165
weighted avg       0.73      0.72      0.69       165

> sigmoid
2 layer
> Accuracy = 0.7575757575757576
              precision    recall  f1-score   support

         0.0       0.74      0.93      0.82       101
         1.0       0.82      0.48      0.61        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.78      0.71      0.72       165
weighted avg       0.77      0.76      0.74       165

> sigmoid
3 layer
> Accuracy = 0.7515151515151515
              precision    recall  f1-score   support

         0.0       0.74      0.92      0.82       101
         1.0       0.79      0.48      0.60        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.77      0.70      0.71       165
weighted avg       0.76      0.75      0.74       165

> relu
1 layer
> Accuracy = 0.7090909090909091
              precision    recall  f1-score   support

         0.0       0.69      0.97      0.80       101
         1.0       0.86      0.30      0.44        64

   micro avg       0.71      0.71      0.71       165
   macro avg       0.77      0.63      0.62       165
weighted avg       0.75      0.71      0.66       165

> relu
2 layer
> Accuracy = 0.703030303030303
              precision    recall  f1-score   support

         0.0       0.68      0.96      0.80       101
         1.0       0.83      0.30      0.44        64

   micro avg       0.70      0.70      0.70       165
   macro avg       0.75      0.63      0.62       165
weighted avg       0.74      0.70      0.66       165

> relu
3 layer
> Accuracy = 0.7515151515151515
              precision    recall  f1-score   support

         0.0       0.78      0.83      0.80       101
         1.0       0.70      0.62      0.66        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.74      0.73      0.73       165
weighted avg       0.75      0.75      0.75       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 0.7454545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> Applying KNN
> n=2
> Accuracy = 0.6424242424242425
              precision    recall  f1-score   support

         0.0       0.63      0.98      0.77       101
         1.0       0.78      0.11      0.19        64

   micro avg       0.64      0.64      0.64       165
   macro avg       0.71      0.54      0.48       165
weighted avg       0.69      0.64      0.55       165

> n=3
> Accuracy = 0.6848484848484848
              precision    recall  f1-score   support

         0.0       0.68      0.92      0.78       101
         1.0       0.71      0.31      0.43        64

   micro avg       0.68      0.68      0.68       165
   macro avg       0.70      0.62      0.61       165
weighted avg       0.69      0.68      0.65       165

> n=4
> Accuracy = 0.6606060606060606
              precision    recall  f1-score   support

         0.0       0.65      0.98      0.78       101
         1.0       0.83      0.16      0.26        64

   micro avg       0.66      0.66      0.66       165
   macro avg       0.74      0.57      0.52       165
weighted avg       0.72      0.66      0.58       165

> n=5
> Accuracy = 0.6484848484848484
              precision    recall  f1-score   support

         0.0       0.65      0.90      0.76       101
         1.0       0.62      0.25      0.36        64

   micro avg       0.65      0.65      0.65       165
   macro avg       0.64      0.58      0.56       165
weighted avg       0.64      0.65      0.60       165

> Applying SVM
> linear
> Accuracy = 0.7696969696969697
              precision    recall  f1-score   support

         0.0       0.75      0.93      0.83       101
         1.0       0.82      0.52      0.63        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.79      0.72      0.73       165
weighted avg       0.78      0.77      0.76       165

> rbf
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 2
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 3
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 4
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> Applying MLP
> sigmoid
1 layer
> Accuracy = 0.7212121212121212
              precision    recall  f1-score   support

         0.0       0.70      0.94      0.81       101
         1.0       0.80      0.38      0.51        64

   micro avg       0.72      0.72      0.72       165
   macro avg       0.75      0.66      0.66       165
weighted avg       0.74      0.72      0.69       165

> sigmoid
2 layer
> Accuracy = 0.7272727272727273
              precision    recall  f1-score   support

         0.0       0.71      0.95      0.81       101
         1.0       0.83      0.38      0.52        64

   micro avg       0.73      0.73      0.73       165
   macro avg       0.77      0.66      0.66       165
weighted avg       0.75      0.73      0.70       165

> sigmoid
3 layer
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> relu
1 layer
> Accuracy = 0.7393939393939394
              precision    recall  f1-score   support

         0.0       0.72      0.93      0.81       101
         1.0       0.80      0.44      0.57        64

   micro avg       0.74      0.74      0.74       165
   macro avg       0.76      0.68      0.69       165
weighted avg       0.75      0.74      0.72       165

> relu
2 layer
> Accuracy = 0.696969696969697
              precision    recall  f1-score   support

         0.0       0.67      0.98      0.80       101
         1.0       0.89      0.25      0.39        64

   micro avg       0.70      0.70      0.70       165
   macro avg       0.78      0.62      0.59       165
weighted avg       0.76      0.70      0.64       165

> relu
3 layer
> Accuracy = 0.7757575757575758
              precision    recall  f1-score   support

         0.0       0.77      0.91      0.83       101
         1.0       0.80      0.56      0.66        64

   micro avg       0.78      0.78      0.78       165
   macro avg       0.78      0.74      0.75       165
weighted avg       0.78      0.78      0.77       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 0.7454545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> Applying KNN
> n=2
> Accuracy = 0.6424242424242425
              precision    recall  f1-score   support

         0.0       0.63      0.98      0.77       101
         1.0       0.78      0.11      0.19        64

   micro avg       0.64      0.64      0.64       165
   macro avg       0.71      0.54      0.48       165
weighted avg       0.69      0.64      0.55       165

> n=3
> Accuracy = 0.6848484848484848
              precision    recall  f1-score   support

         0.0       0.68      0.92      0.78       101
         1.0       0.71      0.31      0.43        64

   micro avg       0.68      0.68      0.68       165
   macro avg       0.70      0.62      0.61       165
weighted avg       0.69      0.68      0.65       165

> n=4
> Accuracy = 0.6606060606060606
              precision    recall  f1-score   support

         0.0       0.65      0.98      0.78       101
         1.0       0.83      0.16      0.26        64

   micro avg       0.66      0.66      0.66       165
   macro avg       0.74      0.57      0.52       165
weighted avg       0.72      0.66      0.58       165

> n=5
> Accuracy = 0.6484848484848484
              precision    recall  f1-score   support

         0.0       0.65      0.90      0.76       101
         1.0       0.62      0.25      0.36        64

   micro avg       0.65      0.65      0.65       165
   macro avg       0.64      0.58      0.56       165
weighted avg       0.64      0.65      0.60       165

> Applying SVM
> linear
> Accuracy = 0.7696969696969697
              precision    recall  f1-score   support

         0.0       0.75      0.93      0.83       101
         1.0       0.82      0.52      0.63        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.79      0.72      0.73       165
weighted avg       0.78      0.77      0.76       165

> rbf
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 2
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 3
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 4
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> Applying MLP
> sigmoid
1 layer
> Accuracy = 0.7272727272727273
              precision    recall  f1-score   support

         0.0       0.72      0.91      0.80       101
         1.0       0.76      0.44      0.55        64

   micro avg       0.73      0.73      0.73       165
   macro avg       0.74      0.67      0.68       165
weighted avg       0.73      0.73      0.71       165

> sigmoid
2 layer
> Accuracy = 0.703030303030303
              precision    recall  f1-score   support

         0.0       0.68      0.96      0.80       101
         1.0       0.83      0.30      0.44        64

   micro avg       0.70      0.70      0.70       165
   macro avg       0.75      0.63      0.62       165
weighted avg       0.74      0.70      0.66       165

> sigmoid
3 layer
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.80      0.81      0.81       101
         1.0       0.70      0.69      0.69        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.75      0.75      0.75       165
weighted avg       0.76      0.76      0.76       165

> relu
1 layer
> Accuracy = 0.6666666666666666
              precision    recall  f1-score   support

         0.0       0.65      0.97      0.78       101
         1.0       0.80      0.19      0.30        64

   micro avg       0.67      0.67      0.67       165
   macro avg       0.73      0.58      0.54       165
weighted avg       0.71      0.67      0.60       165

> relu
2 layer
> Accuracy = 0.7090909090909091
              precision    recall  f1-score   support

         0.0       0.68      0.98      0.80       101
         1.0       0.90      0.28      0.43        64

   micro avg       0.71      0.71      0.71       165
   macro avg       0.79      0.63      0.62       165
weighted avg       0.77      0.71      0.66       165

> relu
3 layer
> Accuracy = 0.7454545454545455
              precision    recall  f1-score   support

         0.0       0.84      0.72      0.78       101
         1.0       0.64      0.78      0.70        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.74      0.75      0.74       165
weighted avg       0.76      0.75      0.75       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 0.7454545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> Applying KNN
> n=2
> Accuracy = 0.6424242424242425
              precision    recall  f1-score   support

         0.0       0.63      0.98      0.77       101
         1.0       0.78      0.11      0.19        64

   micro avg       0.64      0.64      0.64       165
   macro avg       0.71      0.54      0.48       165
weighted avg       0.69      0.64      0.55       165

> n=3
> Accuracy = 0.6848484848484848
              precision    recall  f1-score   support

         0.0       0.68      0.92      0.78       101
         1.0       0.71      0.31      0.43        64

   micro avg       0.68      0.68      0.68       165
   macro avg       0.70      0.62      0.61       165
weighted avg       0.69      0.68      0.65       165

> n=4
> Accuracy = 0.6606060606060606
              precision    recall  f1-score   support

         0.0       0.65      0.98      0.78       101
         1.0       0.83      0.16      0.26        64

   micro avg       0.66      0.66      0.66       165
   macro avg       0.74      0.57      0.52       165
weighted avg       0.72      0.66      0.58       165

> n=5
> Accuracy = 0.6484848484848484
              precision    recall  f1-score   support

         0.0       0.65      0.90      0.76       101
         1.0       0.62      0.25      0.36        64

   micro avg       0.65      0.65      0.65       165
   macro avg       0.64      0.58      0.56       165
weighted avg       0.64      0.65      0.60       165

> Applying SVM
> linear
> Accuracy = 0.7696969696969697
              precision    recall  f1-score   support

         0.0       0.75      0.93      0.83       101
         1.0       0.82      0.52      0.63        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.79      0.72      0.73       165
weighted avg       0.78      0.77      0.76       165

> rbf
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 2
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 3
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 4
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> Applying MLP
> sigmoid
1 layer
> Accuracy = 0.7272727272727273
              precision    recall  f1-score   support

         0.0       0.72      0.91      0.80       101
         1.0       0.76      0.44      0.55        64

   micro avg       0.73      0.73      0.73       165
   macro avg       0.74      0.67      0.68       165
weighted avg       0.73      0.73      0.71       165

> sigmoid
2 layer
> Accuracy = 0.7515151515151515
              precision    recall  f1-score   support

         0.0       0.73      0.93      0.82       101
         1.0       0.81      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.77      0.70      0.71       165
weighted avg       0.76      0.75      0.73       165

> sigmoid
3 layer
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> relu
1 layer
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.75      0.91      0.83       101
         1.0       0.79      0.53      0.64        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.77      0.72      0.73       165
weighted avg       0.77      0.76      0.75       165

> relu
2 layer
> Accuracy = 0.7575757575757576
              precision    recall  f1-score   support

         0.0       0.74      0.93      0.82       101
         1.0       0.82      0.48      0.61        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.78      0.71      0.72       165
weighted avg       0.77      0.76      0.74       165

> relu
3 layer
> Accuracy = 0.703030303030303
              precision    recall  f1-score   support

         0.0       0.68      0.96      0.80       101
         1.0       0.83      0.30      0.44        64

   micro avg       0.70      0.70      0.70       165
   macro avg       0.75      0.63      0.62       165
weighted avg       0.74      0.70      0.66       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 0.7454545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> Applying KNN
> n=2
> Accuracy = 0.6424242424242425
              precision    recall  f1-score   support

         0.0       0.63      0.98      0.77       101
         1.0       0.78      0.11      0.19        64

   micro avg       0.64      0.64      0.64       165
   macro avg       0.71      0.54      0.48       165
weighted avg       0.69      0.64      0.55       165

> n=3
> Accuracy = 0.6848484848484848
              precision    recall  f1-score   support

         0.0       0.68      0.92      0.78       101
         1.0       0.71      0.31      0.43        64

   micro avg       0.68      0.68      0.68       165
   macro avg       0.70      0.62      0.61       165
weighted avg       0.69      0.68      0.65       165

> n=4
> Accuracy = 0.6606060606060606
              precision    recall  f1-score   support

         0.0       0.65      0.98      0.78       101
         1.0       0.83      0.16      0.26        64

   micro avg       0.66      0.66      0.66       165
   macro avg       0.74      0.57      0.52       165
weighted avg       0.72      0.66      0.58       165

> n=5
> Accuracy = 0.6484848484848484
              precision    recall  f1-score   support

         0.0       0.65      0.90      0.76       101
         1.0       0.62      0.25      0.36        64

   micro avg       0.65      0.65      0.65       165
   macro avg       0.64      0.58      0.56       165
weighted avg       0.64      0.65      0.60       165

> Applying SVM
> linear
> Accuracy = 0.7696969696969697
              precision    recall  f1-score   support

         0.0       0.75      0.93      0.83       101
         1.0       0.82      0.52      0.63        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.79      0.72      0.73       165
weighted avg       0.78      0.77      0.76       165

> rbf
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 2
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 3
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 4
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> Applying MLP
> sigmoid
1 layer
> Accuracy = 0.7212121212121212
              precision    recall  f1-score   support

         0.0       0.71      0.93      0.80       101
         1.0       0.78      0.39      0.52        64

   micro avg       0.72      0.72      0.72       165
   macro avg       0.74      0.66      0.66       165
weighted avg       0.74      0.72      0.69       165

> sigmoid
2 layer
> Accuracy = 0.7151515151515152
              precision    recall  f1-score   support

         0.0       0.69      0.96      0.80       101
         1.0       0.84      0.33      0.47        64

   micro avg       0.72      0.72      0.72       165
   macro avg       0.77      0.64      0.64       165
weighted avg       0.75      0.72      0.68       165

> sigmoid
3 layer
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> relu
1 layer
> Accuracy = 0.7151515151515152
              precision    recall  f1-score   support

         0.0       0.69      0.96      0.80       101
         1.0       0.84      0.33      0.47        64

   micro avg       0.72      0.72      0.72       165
   macro avg       0.77      0.64      0.64       165
weighted avg       0.75      0.72      0.68       165

> relu
2 layer
> Accuracy = 0.7757575757575758
              precision    recall  f1-score   support

         0.0       0.76      0.93      0.84       101
         1.0       0.83      0.53      0.65        64

   micro avg       0.78      0.78      0.78       165
   macro avg       0.79      0.73      0.74       165
weighted avg       0.79      0.78      0.76       165

> relu
3 layer
> Accuracy = 0.7333333333333333
              precision    recall  f1-score   support

         0.0       0.81      0.74      0.77       101
         1.0       0.64      0.72      0.68        64

   micro avg       0.73      0.73      0.73       165
   macro avg       0.72      0.73      0.72       165
weighted avg       0.74      0.73      0.74       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 0.7454545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> Applying KNN
> n=2
> Accuracy = 0.6424242424242425
              precision    recall  f1-score   support

         0.0       0.63      0.98      0.77       101
         1.0       0.78      0.11      0.19        64

   micro avg       0.64      0.64      0.64       165
   macro avg       0.71      0.54      0.48       165
weighted avg       0.69      0.64      0.55       165

> n=3
> Accuracy = 0.6848484848484848
              precision    recall  f1-score   support

         0.0       0.68      0.92      0.78       101
         1.0       0.71      0.31      0.43        64

   micro avg       0.68      0.68      0.68       165
   macro avg       0.70      0.62      0.61       165
weighted avg       0.69      0.68      0.65       165

> n=4
> Accuracy = 0.6606060606060606
              precision    recall  f1-score   support

         0.0       0.65      0.98      0.78       101
         1.0       0.83      0.16      0.26        64

   micro avg       0.66      0.66      0.66       165
   macro avg       0.74      0.57      0.52       165
weighted avg       0.72      0.66      0.58       165

> n=5
> Accuracy = 0.6484848484848484
              precision    recall  f1-score   support

         0.0       0.65      0.90      0.76       101
         1.0       0.62      0.25      0.36        64

   micro avg       0.65      0.65      0.65       165
   macro avg       0.64      0.58      0.56       165
weighted avg       0.64      0.65      0.60       165

> Applying SVM
> linear
> Accuracy = 0.7696969696969697
              precision    recall  f1-score   support

         0.0       0.75      0.93      0.83       101
         1.0       0.82      0.52      0.63        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.79      0.72      0.73       165
weighted avg       0.78      0.77      0.76       165

> rbf
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 2
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 3
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 4
> Accuracy = 0.6121212121212121
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> Applying MLP
> sigmoid
1 layer
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.76      0.90      0.82       101
         1.0       0.78      0.55      0.64        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.77      0.72      0.73       165
weighted avg       0.77      0.76      0.75       165

> sigmoid
2 layer
> Accuracy = 0.7212121212121212
              precision    recall  f1-score   support

         0.0       0.70      0.94      0.81       101
         1.0       0.80      0.38      0.51        64

   micro avg       0.72      0.72      0.72       165
   macro avg       0.75      0.66      0.66       165
weighted avg       0.74      0.72      0.69       165

> sigmoid
3 layer
> Accuracy = 0.7272727272727273
              precision    recall  f1-score   support

         0.0       0.71      0.95      0.81       101
         1.0       0.83      0.38      0.52        64

   micro avg       0.73      0.73      0.73       165
   macro avg       0.77      0.66      0.66       165
weighted avg       0.75      0.73      0.70       165

> relu
1 layer
> Accuracy = 0.7090909090909091
              precision    recall  f1-score   support

         0.0       0.70      0.92      0.79       101
         1.0       0.75      0.38      0.50        64

   micro avg       0.71      0.71      0.71       165
   macro avg       0.72      0.65      0.65       165
weighted avg       0.72      0.71      0.68       165

> relu
2 layer
> Accuracy = 0.7151515151515152
              precision    recall  f1-score   support

         0.0       0.69      0.96      0.80       101
         1.0       0.84      0.33      0.47        64

   micro avg       0.72      0.72      0.72       165
   macro avg       0.77      0.64      0.64       165
weighted avg       0.75      0.72      0.68       165

> relu
3 layer
> Accuracy = 0.7212121212121212
              precision    recall  f1-score   support

         0.0       0.91      0.60      0.73       101
         1.0       0.59      0.91      0.72        64

   micro avg       0.72      0.72      0.72       165
   macro avg       0.75      0.76      0.72       165
weighted avg       0.79      0.72      0.72       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 0.7454545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 0.7454545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 0.7454545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 0.7636363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 0.7454545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 76.36363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 74.54545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 75.75757575757575
              precision    recall  f1-score   support

         0.0       0.72      0.98      0.83       101
         1.0       0.93      0.41      0.57        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.69      0.70       165
weighted avg       0.80      0.76      0.73       165

> l2 penalty
> Accuracy = 74.54545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> Applying KNN
> n=2
> Accuracy = 64.24242424242425
              precision    recall  f1-score   support

         0.0       0.63      0.98      0.77       101
         1.0       0.78      0.11      0.19        64

   micro avg       0.64      0.64      0.64       165
   macro avg       0.71      0.54      0.48       165
weighted avg       0.69      0.64      0.55       165

> n=3
> Accuracy = 68.48484848484848
              precision    recall  f1-score   support

         0.0       0.68      0.92      0.78       101
         1.0       0.71      0.31      0.43        64

   micro avg       0.68      0.68      0.68       165
   macro avg       0.70      0.62      0.61       165
weighted avg       0.69      0.68      0.65       165

> n=4
> Accuracy = 66.06060606060606
              precision    recall  f1-score   support

         0.0       0.65      0.98      0.78       101
         1.0       0.83      0.16      0.26        64

   micro avg       0.66      0.66      0.66       165
   macro avg       0.74      0.57      0.52       165
weighted avg       0.72      0.66      0.58       165

> n=5
> Accuracy = 64.84848484848484
              precision    recall  f1-score   support

         0.0       0.65      0.90      0.76       101
         1.0       0.62      0.25      0.36        64

   micro avg       0.65      0.65      0.65       165
   macro avg       0.64      0.58      0.56       165
weighted avg       0.64      0.65      0.60       165

> Applying SVM
> linear
> Accuracy = 76.96969696969697
              precision    recall  f1-score   support

         0.0       0.75      0.93      0.83       101
         1.0       0.82      0.52      0.63        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.79      0.72      0.73       165
weighted avg       0.78      0.77      0.76       165

> rbf
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 2
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 3
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 4
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> Applying MLP
> sigmoid
1 layer
> Accuracy = 71.51515151515152
              precision    recall  f1-score   support

         0.0       0.70      0.95      0.80       101
         1.0       0.81      0.34      0.48        64

   micro avg       0.72      0.72      0.72       165
   macro avg       0.76      0.65      0.64       165
weighted avg       0.74      0.72      0.68       165

> sigmoid
2 layer
> Accuracy = 70.3030303030303
              precision    recall  f1-score   support

         0.0       0.68      0.96      0.80       101
         1.0       0.83      0.30      0.44        64

   micro avg       0.70      0.70      0.70       165
   macro avg       0.75      0.63      0.62       165
weighted avg       0.74      0.70      0.66       165

> sigmoid
3 layer
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> relu
1 layer
> Accuracy = 77.57575757575758
              precision    recall  f1-score   support

         0.0       0.79      0.86      0.82       101
         1.0       0.75      0.64      0.69        64

   micro avg       0.78      0.78      0.78       165
   macro avg       0.77      0.75      0.76       165
weighted avg       0.77      0.78      0.77       165

> relu
2 layer
> Accuracy = 66.66666666666666
              precision    recall  f1-score   support

         0.0       0.65      0.97      0.78       101
         1.0       0.80      0.19      0.30        64

   micro avg       0.67      0.67      0.67       165
   macro avg       0.73      0.58      0.54       165
weighted avg       0.71      0.67      0.60       165

> relu
3 layer
> Accuracy = 72.12121212121212
              precision    recall  f1-score   support

         0.0       0.70      0.96      0.81       101
         1.0       0.85      0.34      0.49        64

   micro avg       0.72      0.72      0.72       165
   macro avg       0.77      0.65      0.65       165
weighted avg       0.76      0.72      0.68       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 76.36363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 74.54545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> Applying KNN
> n=2
> Accuracy = 64.24242424242425
              precision    recall  f1-score   support

         0.0       0.63      0.98      0.77       101
         1.0       0.78      0.11      0.19        64

   micro avg       0.64      0.64      0.64       165
   macro avg       0.71      0.54      0.48       165
weighted avg       0.69      0.64      0.55       165

> n=3
> Accuracy = 68.48484848484848
              precision    recall  f1-score   support

         0.0       0.68      0.92      0.78       101
         1.0       0.71      0.31      0.43        64

   micro avg       0.68      0.68      0.68       165
   macro avg       0.70      0.62      0.61       165
weighted avg       0.69      0.68      0.65       165

> n=4
> Accuracy = 66.06060606060606
              precision    recall  f1-score   support

         0.0       0.65      0.98      0.78       101
         1.0       0.83      0.16      0.26        64

   micro avg       0.66      0.66      0.66       165
   macro avg       0.74      0.57      0.52       165
weighted avg       0.72      0.66      0.58       165

> n=5
> Accuracy = 64.84848484848484
              precision    recall  f1-score   support

         0.0       0.65      0.90      0.76       101
         1.0       0.62      0.25      0.36        64

   micro avg       0.65      0.65      0.65       165
   macro avg       0.64      0.58      0.56       165
weighted avg       0.64      0.65      0.60       165

> Applying SVM
> linear
> Accuracy = 76.96969696969697
              precision    recall  f1-score   support

         0.0       0.75      0.93      0.83       101
         1.0       0.82      0.52      0.63        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.79      0.72      0.73       165
weighted avg       0.78      0.77      0.76       165

> rbf
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 2
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 3
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 4
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> Applying MLP
> sigmoid
1 layer
> Accuracy = 74.54545454545455
              precision    recall  f1-score   support

         0.0       0.74      0.91      0.81       101
         1.0       0.78      0.48      0.60        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.70      0.71       165
weighted avg       0.75      0.75      0.73       165

> sigmoid
2 layer
> Accuracy = 70.3030303030303
              precision    recall  f1-score   support

         0.0       0.69      0.94      0.79       101
         1.0       0.78      0.33      0.46        64

   micro avg       0.70      0.70      0.70       165
   macro avg       0.73      0.63      0.63       165
weighted avg       0.72      0.70      0.67       165

> sigmoid
3 layer
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> relu
1 layer
> Accuracy = 73.33333333333333
              precision    recall  f1-score   support

         0.0       0.72      0.92      0.81       101
         1.0       0.78      0.44      0.56        64

   micro avg       0.73      0.73      0.73       165
   macro avg       0.75      0.68      0.68       165
weighted avg       0.74      0.73      0.71       165

> relu
2 layer
> Accuracy = 70.3030303030303
              precision    recall  f1-score   support

         0.0       0.68      0.97      0.80       101
         1.0       0.86      0.28      0.42        64

   micro avg       0.70      0.70      0.70       165
   macro avg       0.77      0.63      0.61       165
weighted avg       0.75      0.70      0.65       165

> relu
3 layer
> Accuracy = 75.75757575757575
              precision    recall  f1-score   support

         0.0       0.74      0.93      0.82       101
         1.0       0.82      0.48      0.61        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.78      0.71      0.72       165
weighted avg       0.77      0.76      0.74       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 76.36363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 74.54545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> Applying KNN
> n=2
> Accuracy = 64.24242424242425
              precision    recall  f1-score   support

         0.0       0.63      0.98      0.77       101
         1.0       0.78      0.11      0.19        64

   micro avg       0.64      0.64      0.64       165
   macro avg       0.71      0.54      0.48       165
weighted avg       0.69      0.64      0.55       165

> n=3
> Accuracy = 68.48484848484848
              precision    recall  f1-score   support

         0.0       0.68      0.92      0.78       101
         1.0       0.71      0.31      0.43        64

   micro avg       0.68      0.68      0.68       165
   macro avg       0.70      0.62      0.61       165
weighted avg       0.69      0.68      0.65       165

> n=4
> Accuracy = 66.06060606060606
              precision    recall  f1-score   support

         0.0       0.65      0.98      0.78       101
         1.0       0.83      0.16      0.26        64

   micro avg       0.66      0.66      0.66       165
   macro avg       0.74      0.57      0.52       165
weighted avg       0.72      0.66      0.58       165

> n=5
> Accuracy = 64.84848484848484
              precision    recall  f1-score   support

         0.0       0.65      0.90      0.76       101
         1.0       0.62      0.25      0.36        64

   micro avg       0.65      0.65      0.65       165
   macro avg       0.64      0.58      0.56       165
weighted avg       0.64      0.65      0.60       165

> Applying SVM
> linear
> Accuracy = 76.96969696969697
              precision    recall  f1-score   support

         0.0       0.75      0.93      0.83       101
         1.0       0.82      0.52      0.63        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.79      0.72      0.73       165
weighted avg       0.78      0.77      0.76       165

> rbf
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 2
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 3
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 4
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> Applying MLP
> sigmoid
1 layer
> Accuracy = 73.33333333333333
              precision    recall  f1-score   support

         0.0       0.73      0.90      0.81       101
         1.0       0.75      0.47      0.58        64

   micro avg       0.73      0.73      0.73       165
   macro avg       0.74      0.68      0.69       165
weighted avg       0.74      0.73      0.72       165

> sigmoid
2 layer
> Accuracy = 70.3030303030303
              precision    recall  f1-score   support

         0.0       0.68      0.96      0.80       101
         1.0       0.83      0.30      0.44        64

   micro avg       0.70      0.70      0.70       165
   macro avg       0.75      0.63      0.62       165
weighted avg       0.74      0.70      0.66       165

> sigmoid
3 layer
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> relu
1 layer
> Accuracy = 75.75757575757575
              precision    recall  f1-score   support

         0.0       0.74      0.92      0.82       101
         1.0       0.80      0.50      0.62        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.77      0.71      0.72       165
weighted avg       0.77      0.76      0.74       165

> relu
2 layer
> Accuracy = 75.15151515151514
              precision    recall  f1-score   support

         0.0       0.80      0.79      0.80       101
         1.0       0.68      0.69      0.68        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.74      0.74      0.74       165
weighted avg       0.75      0.75      0.75       165

> relu
3 layer
> Accuracy = 76.96969696969697
              precision    recall  f1-score   support

         0.0       0.77      0.89      0.83       101
         1.0       0.77      0.58      0.66        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.77      0.73      0.74       165
weighted avg       0.77      0.77      0.76       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 76.36363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 74.54545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 75.75757575757575
              precision    recall  f1-score   support

         0.0       0.72      0.98      0.83       101
         1.0       0.93      0.41      0.57        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.69      0.70       165
weighted avg       0.80      0.76      0.73       165

> l2 penalty
> Accuracy = 74.54545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> Applying KNN
> n=2
> Accuracy = 64.24242424242425
              precision    recall  f1-score   support

         0.0       0.63      0.98      0.77       101
         1.0       0.78      0.11      0.19        64

   micro avg       0.64      0.64      0.64       165
   macro avg       0.71      0.54      0.48       165
weighted avg       0.69      0.64      0.55       165

> n=3
> Accuracy = 68.48484848484848
              precision    recall  f1-score   support

         0.0       0.68      0.92      0.78       101
         1.0       0.71      0.31      0.43        64

   micro avg       0.68      0.68      0.68       165
   macro avg       0.70      0.62      0.61       165
weighted avg       0.69      0.68      0.65       165

> n=4
> Accuracy = 66.06060606060606
              precision    recall  f1-score   support

         0.0       0.65      0.98      0.78       101
         1.0       0.83      0.16      0.26        64

   micro avg       0.66      0.66      0.66       165
   macro avg       0.74      0.57      0.52       165
weighted avg       0.72      0.66      0.58       165

> n=5
> Accuracy = 64.84848484848484
              precision    recall  f1-score   support

         0.0       0.65      0.90      0.76       101
         1.0       0.62      0.25      0.36        64

   micro avg       0.65      0.65      0.65       165
   macro avg       0.64      0.58      0.56       165
weighted avg       0.64      0.65      0.60       165

> Applying SVM
> linear
> Accuracy = 76.96969696969697
              precision    recall  f1-score   support

         0.0       0.75      0.93      0.83       101
         1.0       0.82      0.52      0.63        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.79      0.72      0.73       165
weighted avg       0.78      0.77      0.76       165

> rbf
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 2
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 3
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 4
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> Applying MLP
> sigmoid
1 layer
> Accuracy = 71.51515151515152
              precision    recall  f1-score   support

         0.0       0.70      0.95      0.80       101
         1.0       0.81      0.34      0.48        64

   micro avg       0.72      0.72      0.72       165
   macro avg       0.76      0.65      0.64       165
weighted avg       0.74      0.72      0.68       165

> sigmoid
2 layer
> Accuracy = 74.54545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.93      0.82       101
         1.0       0.81      0.45      0.58        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.77      0.69      0.70       165
weighted avg       0.76      0.75      0.73       165

> sigmoid
3 layer
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> relu
1 layer
> Accuracy = 75.75757575757575
              precision    recall  f1-score   support

         0.0       0.74      0.92      0.82       101
         1.0       0.80      0.50      0.62        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.77      0.71      0.72       165
weighted avg       0.77      0.76      0.74       165

> relu
2 layer
> Accuracy = 75.15151515151514
              precision    recall  f1-score   support

         0.0       0.78      0.82      0.80       101
         1.0       0.69      0.64      0.67        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.74      0.73      0.73       165
weighted avg       0.75      0.75      0.75       165

> relu
3 layer
> Accuracy = 75.75757575757575
              precision    recall  f1-score   support

         0.0       0.77      0.85      0.81       101
         1.0       0.72      0.61      0.66        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.75      0.73      0.74       165
weighted avg       0.75      0.76      0.75       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 76.36363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 74.54545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 76.36363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 74.54545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 76.36363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 74.54545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 76.36363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 74.54545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 76.36363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 74.54545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> Applying KNN
> n=2
> Accuracy = 64.24242424242425
              precision    recall  f1-score   support

         0.0       0.63      0.98      0.77       101
         1.0       0.78      0.11      0.19        64

   micro avg       0.64      0.64      0.64       165
   macro avg       0.71      0.54      0.48       165
weighted avg       0.69      0.64      0.55       165

> n=3
> Accuracy = 68.48484848484848
              precision    recall  f1-score   support

         0.0       0.68      0.92      0.78       101
         1.0       0.71      0.31      0.43        64

   micro avg       0.68      0.68      0.68       165
   macro avg       0.70      0.62      0.61       165
weighted avg       0.69      0.68      0.65       165

> n=4
> Accuracy = 66.06060606060606
              precision    recall  f1-score   support

         0.0       0.65      0.98      0.78       101
         1.0       0.83      0.16      0.26        64

   micro avg       0.66      0.66      0.66       165
   macro avg       0.74      0.57      0.52       165
weighted avg       0.72      0.66      0.58       165

> n=5
> Accuracy = 64.84848484848484
              precision    recall  f1-score   support

         0.0       0.65      0.90      0.76       101
         1.0       0.62      0.25      0.36        64

   micro avg       0.65      0.65      0.65       165
   macro avg       0.64      0.58      0.56       165
weighted avg       0.64      0.65      0.60       165

> Applying SVM
> linear
> Accuracy = 76.96969696969697
              precision    recall  f1-score   support

         0.0       0.75      0.93      0.83       101
         1.0       0.82      0.52      0.63        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.79      0.72      0.73       165
weighted avg       0.78      0.77      0.76       165

> rbf
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 2
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 3
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 4
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> Applying MLP
> sigmoid
1 layer
> Accuracy = 72.12121212121212
              precision    recall  f1-score   support

         0.0       0.71      0.92      0.80       101
         1.0       0.76      0.41      0.53        64

   micro avg       0.72      0.72      0.72       165
   macro avg       0.74      0.66      0.67       165
weighted avg       0.73      0.72      0.70       165

> sigmoid
2 layer
> Accuracy = 77.57575757575758
              precision    recall  f1-score   support

         0.0       0.78      0.89      0.83       101
         1.0       0.78      0.59      0.67        64

   micro avg       0.78      0.78      0.78       165
   macro avg       0.78      0.74      0.75       165
weighted avg       0.78      0.78      0.77       165

> sigmoid
3 layer
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> relu
1 layer
> Accuracy = 73.33333333333333
              precision    recall  f1-score   support

         0.0       0.72      0.91      0.81       101
         1.0       0.76      0.45      0.57        64

   micro avg       0.73      0.73      0.73       165
   macro avg       0.74      0.68      0.69       165
weighted avg       0.74      0.73      0.71       165

> relu
2 layer
> Accuracy = 76.36363636363637
              precision    recall  f1-score   support

         0.0       0.82      0.78      0.80       101
         1.0       0.68      0.73      0.71        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.75      0.76      0.75       165
weighted avg       0.77      0.76      0.77       165

> relu
3 layer
> Accuracy = 75.75757575757575
              precision    recall  f1-score   support

         0.0       0.81      0.79      0.80       101
         1.0       0.68      0.70      0.69        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.74      0.75      0.75       165
weighted avg       0.76      0.76      0.76       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 76.36363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 74.54545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 76.36363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 74.54545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 76.36363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 74.54545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> Applying KNN
> n=2
> Accuracy = 64.24242424242425
              precision    recall  f1-score   support

         0.0       0.63      0.98      0.77       101
         1.0       0.78      0.11      0.19        64

   micro avg       0.64      0.64      0.64       165
   macro avg       0.71      0.54      0.48       165
weighted avg       0.69      0.64      0.55       165

> n=3
> Accuracy = 68.48484848484848
              precision    recall  f1-score   support

         0.0       0.68      0.92      0.78       101
         1.0       0.71      0.31      0.43        64

   micro avg       0.68      0.68      0.68       165
   macro avg       0.70      0.62      0.61       165
weighted avg       0.69      0.68      0.65       165

> n=4
> Accuracy = 66.06060606060606
              precision    recall  f1-score   support

         0.0       0.65      0.98      0.78       101
         1.0       0.83      0.16      0.26        64

   micro avg       0.66      0.66      0.66       165
   macro avg       0.74      0.57      0.52       165
weighted avg       0.72      0.66      0.58       165

> n=5
> Accuracy = 64.84848484848484
              precision    recall  f1-score   support

         0.0       0.65      0.90      0.76       101
         1.0       0.62      0.25      0.36        64

   micro avg       0.65      0.65      0.65       165
   macro avg       0.64      0.58      0.56       165
weighted avg       0.64      0.65      0.60       165

> Applying SVM
> linear
> Accuracy = 76.96969696969697
              precision    recall  f1-score   support

         0.0       0.75      0.93      0.83       101
         1.0       0.82      0.52      0.63        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.79      0.72      0.73       165
weighted avg       0.78      0.77      0.76       165

> rbf
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 2
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 3
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 4
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> Applying MLP
> sigmoid
1 layer
> Accuracy = 71.51515151515152
              precision    recall  f1-score   support

         0.0       0.70      0.93      0.80       101
         1.0       0.77      0.38      0.51        64

   micro avg       0.72      0.72      0.72       165
   macro avg       0.74      0.65      0.65       165
weighted avg       0.73      0.72      0.69       165

> sigmoid
2 layer
> Accuracy = 76.96969696969697
              precision    recall  f1-score   support

         0.0       0.76      0.91      0.83       101
         1.0       0.80      0.55      0.65        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.78      0.73      0.74       165
weighted avg       0.77      0.77      0.76       165

> sigmoid
3 layer
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> relu
1 layer
> Accuracy = 72.12121212121212
              precision    recall  f1-score   support

         0.0       0.71      0.93      0.80       101
         1.0       0.78      0.39      0.52        64

   micro avg       0.72      0.72      0.72       165
   macro avg       0.74      0.66      0.66       165
weighted avg       0.74      0.72      0.69       165

> relu
2 layer
> Accuracy = 67.27272727272727
              precision    recall  f1-score   support

         0.0       0.66      0.97      0.78       101
         1.0       0.81      0.20      0.33        64

   micro avg       0.67      0.67      0.67       165
   macro avg       0.74      0.59      0.55       165
weighted avg       0.72      0.67      0.61       165

> relu
3 layer
> Accuracy = 67.87878787878789
              precision    recall  f1-score   support

         0.0       0.66      0.97      0.79       101
         1.0       0.82      0.22      0.35        64

   micro avg       0.68      0.68      0.68       165
   macro avg       0.74      0.59      0.57       165
weighted avg       0.72      0.68      0.62       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 76.36363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 74.54545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> Applying KNN
> n=2
> Accuracy = 64.24242424242425
              precision    recall  f1-score   support

         0.0       0.63      0.98      0.77       101
         1.0       0.78      0.11      0.19        64

   micro avg       0.64      0.64      0.64       165
   macro avg       0.71      0.54      0.48       165
weighted avg       0.69      0.64      0.55       165

> n=3
> Accuracy = 68.48484848484848
              precision    recall  f1-score   support

         0.0       0.68      0.92      0.78       101
         1.0       0.71      0.31      0.43        64

   micro avg       0.68      0.68      0.68       165
   macro avg       0.70      0.62      0.61       165
weighted avg       0.69      0.68      0.65       165

> n=4
> Accuracy = 66.06060606060606
              precision    recall  f1-score   support

         0.0       0.65      0.98      0.78       101
         1.0       0.83      0.16      0.26        64

   micro avg       0.66      0.66      0.66       165
   macro avg       0.74      0.57      0.52       165
weighted avg       0.72      0.66      0.58       165

> n=5
> Accuracy = 64.84848484848484
              precision    recall  f1-score   support

         0.0       0.65      0.90      0.76       101
         1.0       0.62      0.25      0.36        64

   micro avg       0.65      0.65      0.65       165
   macro avg       0.64      0.58      0.56       165
weighted avg       0.64      0.65      0.60       165

> Applying SVM
> linear
> Accuracy = 76.96969696969697
              precision    recall  f1-score   support

         0.0       0.75      0.93      0.83       101
         1.0       0.82      0.52      0.63        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.79      0.72      0.73       165
weighted avg       0.78      0.77      0.76       165

> rbf
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 2
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 3
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 4
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> Applying MLP
> sigmoid
1 layer
> Accuracy = 72.12121212121212
              precision    recall  f1-score   support

         0.0       0.70      0.95      0.81       101
         1.0       0.82      0.36      0.50        64

   micro avg       0.72      0.72      0.72       165
   macro avg       0.76      0.65      0.65       165
weighted avg       0.75      0.72      0.69       165

> sigmoid
2 layer
> Accuracy = 76.96969696969697
              precision    recall  f1-score   support

         0.0       0.81      0.81      0.81       101
         1.0       0.70      0.70      0.70        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.76      0.76      0.76       165
weighted avg       0.77      0.77      0.77       165

> sigmoid
3 layer
> Accuracy = 72.12121212121212
              precision    recall  f1-score   support

         0.0       0.70      0.95      0.81       101
         1.0       0.82      0.36      0.50        64

   micro avg       0.72      0.72      0.72       165
   macro avg       0.76      0.65      0.65       165
weighted avg       0.75      0.72      0.69       165

> relu
1 layer
> Accuracy = 73.33333333333333
              precision    recall  f1-score   support

         0.0       0.71      0.94      0.81       101
         1.0       0.81      0.41      0.54        64

   micro avg       0.73      0.73      0.73       165
   macro avg       0.76      0.67      0.68       165
weighted avg       0.75      0.73      0.71       165

> relu
2 layer
> Accuracy = 75.75757575757575
              precision    recall  f1-score   support

         0.0       0.75      0.91      0.82       101
         1.0       0.79      0.52      0.62        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.77      0.71      0.72       165
weighted avg       0.76      0.76      0.74       165

> relu
3 layer
> Accuracy = 74.54545454545455
              precision    recall  f1-score   support

         0.0       0.79      0.80      0.79       101
         1.0       0.68      0.66      0.67        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.73      0.73      0.73       165
weighted avg       0.74      0.75      0.74       165

> We work on hgpca classification
> Process initiated - Building dataset
> Reading COSMIC Cancer Gene Census
> Number of COSMIC Cancer Catalogue: 3946

> Reading CIVIC Cancer Gene Census
> Number of Civic Cancer Catalogue: 366

> Reading Methylation data of TCGA PRAD
> Number of Genes: 20111 | Number of Patients: 549
> Preprocessing Methylation data
> Number of Genes after processing: 821

> not_severe_group.shape (335, 822)
> severe_group.shape (214, 822)
> Saving training and testing data
> Processing completed!
> Applying Logistic Regression
> l1 penalty
> Accuracy = 76.36363636363637
              precision    recall  f1-score   support

         0.0       0.73      0.98      0.84       101
         1.0       0.93      0.42      0.58        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.83      0.70      0.71       165
weighted avg       0.81      0.76      0.74       165

> l2 penalty
> Accuracy = 74.54545454545455
              precision    recall  f1-score   support

         0.0       0.73      0.92      0.82       101
         1.0       0.79      0.47      0.59        64

   micro avg       0.75      0.75      0.75       165
   macro avg       0.76      0.69      0.70       165
weighted avg       0.75      0.75      0.73       165

> Applying KNN
> n=2
> Accuracy = 64.24242424242425
              precision    recall  f1-score   support

         0.0       0.63      0.98      0.77       101
         1.0       0.78      0.11      0.19        64

   micro avg       0.64      0.64      0.64       165
   macro avg       0.71      0.54      0.48       165
weighted avg       0.69      0.64      0.55       165

> n=3
> Accuracy = 68.48484848484848
              precision    recall  f1-score   support

         0.0       0.68      0.92      0.78       101
         1.0       0.71      0.31      0.43        64

   micro avg       0.68      0.68      0.68       165
   macro avg       0.70      0.62      0.61       165
weighted avg       0.69      0.68      0.65       165

> n=4
> Accuracy = 66.06060606060606
              precision    recall  f1-score   support

         0.0       0.65      0.98      0.78       101
         1.0       0.83      0.16      0.26        64

   micro avg       0.66      0.66      0.66       165
   macro avg       0.74      0.57      0.52       165
weighted avg       0.72      0.66      0.58       165

> n=5
> Accuracy = 64.84848484848484
              precision    recall  f1-score   support

         0.0       0.65      0.90      0.76       101
         1.0       0.62      0.25      0.36        64

   micro avg       0.65      0.65      0.65       165
   macro avg       0.64      0.58      0.56       165
weighted avg       0.64      0.65      0.60       165

> Applying SVM
> linear
> Accuracy = 76.96969696969697
              precision    recall  f1-score   support

         0.0       0.75      0.93      0.83       101
         1.0       0.82      0.52      0.63        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.79      0.72      0.73       165
weighted avg       0.78      0.77      0.76       165

> rbf
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 2
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 3
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> poly 4
> Accuracy = 61.212121212121204
              precision    recall  f1-score   support

         0.0       0.61      1.00      0.76       101
         1.0       0.00      0.00      0.00        64

   micro avg       0.61      0.61      0.61       165
   macro avg       0.31      0.50      0.38       165
weighted avg       0.37      0.61      0.46       165

> Applying MLP
> sigmoid
1 layer
> Accuracy = 72.12121212121212
              precision    recall  f1-score   support

         0.0       0.71      0.93      0.80       101
         1.0       0.78      0.39      0.52        64

   micro avg       0.72      0.72      0.72       165
   macro avg       0.74      0.66      0.66       165
weighted avg       0.74      0.72      0.69       165

> sigmoid
2 layer
> Accuracy = 73.33333333333333
              precision    recall  f1-score   support

         0.0       0.71      0.94      0.81       101
         1.0       0.81      0.41      0.54        64

   micro avg       0.73      0.73      0.73       165
   macro avg       0.76      0.67      0.68       165
weighted avg       0.75      0.73      0.71       165

> tanh
1 layer
> Accuracy = 76.36363636363637
              precision    recall  f1-score   support

         0.0       0.77      0.87      0.82       101
         1.0       0.75      0.59      0.66        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.76      0.73      0.74       165
weighted avg       0.76      0.76      0.76       165

> tanh
2 layer
> Accuracy = 76.96969696969697
              precision    recall  f1-score   support

         0.0       0.81      0.82      0.81       101
         1.0       0.71      0.69      0.70        64

   micro avg       0.77      0.77      0.77       165
   macro avg       0.76      0.75      0.76       165
weighted avg       0.77      0.77      0.77       165

> relu
1 layer
> Accuracy = 70.9090909090909
              precision    recall  f1-score   support

         0.0       0.70      0.91      0.79       101
         1.0       0.74      0.39      0.51        64

   micro avg       0.71      0.71      0.71       165
   macro avg       0.72      0.65      0.65       165
weighted avg       0.72      0.71      0.68       165

> relu
2 layer
> Accuracy = 76.36363636363637
              precision    recall  f1-score   support

         0.0       0.84      0.75      0.80       101
         1.0       0.67      0.78      0.72        64

   micro avg       0.76      0.76      0.76       165
   macro avg       0.76      0.77      0.76       165
weighted avg       0.78      0.76      0.77       165

